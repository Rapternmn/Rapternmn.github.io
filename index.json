[{"content":"üß† 1. What is a Loss Function? (High-Level Overview) A loss function measures the difference between the model\u0026rsquo;s prediction and the ground truth.\nIt answers:\n‚û° How wrong is the model? ‚û° In which direction should the parameters change? During training, optimization tries to minimize the loss by updating weights (using gradients).\nLoss ‚Üí Gradient ‚Üí Weight Update ‚Üí Better Predictions.\nTable of Contents What is a Loss Function? Why do we need Loss Functions? How Are Loss Functions Used in Training? Types of Loss Functions in Deep Learning Regression Losses Classification Losses Ranking / Metric Learning Losses Sequence Modeling Losses Image Losses Generative / Autoencoder Losses Reinforcement Learning Losses Autoencoder \u0026amp; Reconstruction Losses Summary: Quick Reference Guide üå≥ 2. Why do we need Loss Functions? ‚úîÔ∏è They quantify model error Without a loss, we cannot measure how good or bad the model is.\n‚úîÔ∏è They guide backpropagation The derivative of the loss tells each weight:\n\u0026ldquo;Increase or decrease to reduce error?\u0026rdquo;\n‚úîÔ∏è Different problems need different loss surfaces Example:\nClassification ‚Üí probability losses Regression ‚Üí distance losses Ranking ‚Üí pairwise or listwise losses Sequence ‚Üí token-level cross entropy Generative Models ‚Üí reconstruction losses, KL divergence, adversarial losses üß∞ How Are Loss Functions Used in Training? Training Loop:\nForward Pass: Take input x ‚Üí model predicts ≈∑ = f(x; Œ∏) Compute Loss: Calculate L(y, ≈∑) where y is ground truth Backward Pass: Compute gradients ‚àÇL/‚àÇŒ∏ using backpropagation Update Weights: Optimizer updates parameters: Œ∏ ‚Üê Œ∏ - Œ± * ‚àáL Repeat: Iterate over batches and epochs until convergence Key Components:\nLoss Function: Quantifies error Gradient: Direction of steepest increase in loss Optimizer: Algorithm to update weights (SGD, Adam, etc.) Learning Rate: Step size for weight updates Mathematical Formulation:\nŒ∏* = argmin_Œ∏ E[L(y, f(x; Œ∏))] The goal is to find parameters Œ∏ that minimize expected loss over the data distribution.\nüß© 3. Types of Loss Functions in Deep Learning We categorize them into 8 main groups.\nüü¶ 1. Regression Losses Used when predicting continuous values.\nMean Squared Error (MSE) Formula:\nL_MSE = (1/n) * Œ£(y_i - ≈∑_i)¬≤ Properties:\nPenalizes large errors more heavily (quadratic penalty) Differentiable everywhere Sensitive to outliers Gradient: ‚àÇL/‚àÇ≈∑ = -2(y - ≈∑) When to use:\nErrors are normally distributed Large errors should be heavily penalized Smooth gradients are needed Limitations:\nCan be dominated by outliers May converge slowly near the optimum Mean Absolute Error (MAE) Formula:\nL_MAE = (1/n) * Œ£|y_i - ≈∑_i| Properties:\nLinear penalty for all errors Robust to outliers Less sensitive to large errors than MSE Gradient: ‚àÇL/‚àÇ≈∑ = -sign(y - ≈∑) (not smooth at zero) When to use:\nOutliers are present in the data All errors should be treated equally Need robustness to noise Limitations:\nNot differentiable at zero May converge slower than MSE Huber Loss Formula:\nL_Huber = { (1/2) * (y - ≈∑)¬≤ if |y - ≈∑| ‚â§ Œ¥ Œ¥ * |y - ≈∑| - (1/2) * Œ¥¬≤ if |y - ≈∑| \u0026gt; Œ¥ } Where Œ¥ (delta) is a hyperparameter (typically 1.0).\nProperties:\nCombines MSE (for small errors) and MAE (for large errors) Smooth and differentiable everywhere Robust to outliers like MAE Gradient: ‚àÇL/‚àÇ≈∑ = -(y - ≈∑) if |y - ≈∑| ‚â§ Œ¥ ‚àÇL/‚àÇ≈∑ = -Œ¥ * sign(y - ≈∑) if |y - ≈∑| \u0026gt; Œ¥ When to use:\nWant balanced behavior between MSE and MAE Need smooth gradients but also robustness Common in robust regression tasks üü• 2. Classification Losses Used when predicting discrete labels.\nBinary Cross Entropy (BCE) Formula:\nL_BCE = -[y * log(p) + (1-y) * log(1-p)] For a batch of n samples:\nL_BCE = -(1/n) * Œ£[y_i * log(p_i) + (1-y_i) * log(1-p_i)] Properties:\nDerived from maximum likelihood estimation Works with sigmoid activation (outputs probabilities) Gradient: ‚àÇL/‚àÇp = -(y/p) + (1-y)/(1-p) = (p - y)/(p(1-p)) Well-suited for binary classification When to use:\nBinary classification problems Output layer uses sigmoid activation Need probabilistic outputs Note: Add epsilon (Œµ) to log arguments to avoid log(0) = -‚àû\nCategorical Cross Entropy (CCE) Formula:\nL_CCE = -Œ£(y_i * log(p_i)) For a batch:\nL_CCE = -(1/n) * Œ£ Œ£(y_ij * log(p_ij)) where i indexes samples and j indexes classes.\nProperties:\nRequires one-hot encoded labels Works with softmax activation Gradient: ‚àÇL/‚àÇp_i = -y_i/p_i Minimizes the KL divergence between true and predicted distributions When to use:\nMulti-class classification Output layer uses softmax activation Labels are one-hot encoded Sparse Categorical Cross Entropy Formula:\nL_Sparse_CCE = -log(p_k) where k is the true class index (integer label, not one-hot).\nProperties:\nSame as CCE but accepts integer labels instead of one-hot More memory efficient (no need to one-hot encode) Computationally equivalent to CCE When to use:\nMulti-class classification with integer labels Want to avoid one-hot encoding overhead Large number of classes (memory savings) Focal Loss Formula:\nFL = -Œ± * (1 - p_t)^Œ≥ * log(p_t) Where:\np_t = p if y = 1, else p_t = 1 - p Œ± (alpha) = weighting factor for rare class (typically 0.25) Œ≥ (gamma) = focusing parameter (typically 2.0) Properties:\nAddresses class imbalance by down-weighting easy examples (1 - p_t)^Œ≥ term reduces loss contribution from well-classified examples When Œ≥ = 0, reduces to standard cross-entropy Higher Œ≥ focuses more on hard examples When to use:\nHighly imbalanced datasets Object detection (many background vs few objects) Need to focus on hard-to-classify examples Example: In object detection, Œ≥=2 means easy negatives contribute 100x less to loss than hard examples.\nüü© 3. Ranking / Metric Learning Losses Used for retrieval, search, similarity tasks.\nContrastive Loss Formula:\nL_contrastive = { (1/2) * d¬≤ if y = 1 (similar) (1/2) * max(0, margin - d)¬≤ if y = 0 (dissimilar) } Where:\nd = ||f(x_a) - f(x_b)|| (distance between embeddings) margin = minimum distance for dissimilar pairs (hyperparameter, typically 1.0) Properties:\nPulls similar pairs together, pushes dissimilar pairs apart Used in Siamese networks Requires pairs of samples (similar/dissimilar labels) Gradient encourages embedding space separation When to use:\nLearning similarity metrics Siamese network architectures Face recognition, signature verification Need to learn meaningful embeddings Triplet Loss Formula:\nL_triplet = max(0, d(a,p) - d(a,n) + margin) Where:\na = anchor sample p = positive sample (same class as anchor) n = negative sample (different class from anchor) d(a,p) = distance between anchor and positive d(a,n) = distance between anchor and negative margin = minimum desired separation (typically 0.2-1.0) Properties:\nEnsures: d(a,p) + margin \u0026lt; d(a,n) Requires triplets: (anchor, positive, negative) Used in FaceNet, person re-identification More efficient than contrastive (one loss per triplet vs two pairs) When to use:\nFace recognition (FaceNet) Person re-identification Learning embeddings where relative distances matter Need to ensure positive is closer than negative by margin Note: Hard negative mining is crucial - easy triplets contribute zero loss.\nPairwise Ranking / Hinge Loss Formula:\nL_hinge = max(0, margin - (s_pos - s_neg)) Where:\ns_pos = score for positive/relevant item s_neg = score for negative/irrelevant item margin = desired score difference (typically 1.0) Properties:\nUsed in Learning-to-Rank Encourages positive items to score higher than negatives Zero loss when score difference exceeds margin Common in recommendation systems, search ranking When to use:\nLearning-to-Rank problems Recommendation systems Search result ranking Need to order items by relevance üü™ 4. Sequence Modeling Losses Used in NLP, Transformers, LLMs.\nCross Entropy Loss (Token-level) Formula:\nL_CE = -(1/T) * Œ£ log(p(y_t | x_\u0026lt;t)) Where:\nT = sequence length y_t = true token at position t x_\u0026lt;t = context up to position t Properties:\nStandard loss for language modeling Applied per token in sequence Used in GPT, BERT, T5, translation models Same as categorical cross-entropy but applied token-wise When to use:\nLanguage modeling (next token prediction) Machine translation Text generation Any autoregressive sequence model Note: Often combined with teacher forcing during training.\nLabel Smoothing Loss Formula:\nL_smooth = -(1/T) * Œ£ [y_t * log(p_t) + (1-y_t) * log(1-p_t)] Where the true distribution is modified:\nHard labels: y_t = 1 for true class, 0 otherwise Smoothed labels: y_t = (1 - Œ±) for true class, Œ±/(K-1) for others Œ± = smoothing factor (typically 0.1) K = number of classes Properties:\nPrevents overconfidence (logits don\u0026rsquo;t become too extreme) Regularization effect Improves generalization Used in BERT, GPT-2, many modern LLMs When to use:\nModels becoming overconfident Need better calibration Want regularization without dropout Large language models Example: With Œ±=0.1 and K=1000, true class gets 0.9, others get 0.1/999 ‚âà 0.0001\nKL Divergence Formula:\nKL(P || Q) = Œ£ P(x) * log(P(x) / Q(x)) For continuous distributions:\nKL(P || Q) = ‚à´ p(x) * log(p(x) / q(x)) dx Properties:\nMeasures difference between two probability distributions Asymmetric: KL(P||Q) ‚â† KL(Q||P) Always non-negative, zero when P = Q Used in VAEs (regularization term) and knowledge distillation When to use:\nVAEs: Regularize latent space to match prior (typically N(0,1)) Knowledge Distillation: Match student to teacher distribution Variational inference: Approximate posterior to prior VAE Example:\nL_VAE = Reconstruction_Loss + Œ≤ * KL(q(z|x) || p(z)) where Œ≤ controls regularization strength (Œ≤-VAE).\nüü´ 5. Image Losses Dice Loss Formula:\nDice_Score = (2 * |X ‚à© Y|) / (|X| + |Y|) Dice_Loss = 1 - Dice_Score In terms of predictions and ground truth:\nDice_Loss = 1 - (2 * Œ£(p_i * y_i) + Œµ) / (Œ£(p_i) + Œ£(y_i) + Œµ) Where:\np_i = predicted probability for pixel i y_i = ground truth label for pixel i (0 or 1) Œµ = small constant to avoid division by zero (typically 1e-5) Properties:\nMeasures overlap between predicted and true masks Range: [0, 1], where 0 = perfect overlap Handles class imbalance well (focuses on foreground) Differentiable approximation of Dice coefficient When to use:\nMedical image segmentation Imbalanced segmentation tasks (small objects) UNet, DeepLab architectures When IoU is the evaluation metric Advantages:\nLess sensitive to class imbalance than BCE Directly optimizes overlap metric IoU Loss (Jaccard Loss) Formula:\nIoU = |X ‚à© Y| / |X ‚à™ Y| IoU_Loss = 1 - IoU In terms of predictions:\nIoU_Loss = 1 - (Œ£(p_i * y_i) + Œµ) / (Œ£(p_i + y_i - p_i * y_i) + Œµ) Properties:\nIntersection over Union metric Range: [0, 1] Standard evaluation metric for segmentation Penalizes both false positives and false negatives When to use:\nSegmentation tasks where IoU is the evaluation metric Need to directly optimize IoU Object detection (bounding box IoU) Comparison with Dice:\nIoU is more strict (penalizes more for errors) Dice is more forgiving for small errors Both handle class imbalance Combined Losses: BCE + Dice / BCE + IoU Formula:\nL_combined = Œ± * L_BCE + (1 - Œ±) * L_Dice Or:\nL_combined = Œ± * L_BCE + (1 - Œ±) * L_IoU Where Œ± is a weighting factor (typically 0.5).\nProperties:\nCombines pixel-wise (BCE) and region-wise (Dice/IoU) losses BCE provides stable gradients Dice/IoU directly optimizes overlap metric Common in UNet, DeepLab, and modern segmentation models When to use:\nMedical image segmentation UNet-based architectures Want benefits of both losses Need stable training with direct metric optimization Typical values: Œ± = 0.5 (equal weighting) or Œ± = 0.7 (more weight on BCE)\nüüß 6. Generative / Autoencoder Losses Minimax Loss (Original GAN) Formula:\nGenerator Loss:\nL_G = -E[log(D(G(z)))] Discriminator Loss:\nL_D = -E[log(D(x))] - E[log(1 - D(G(z)))] Where:\nD(x) = discriminator output for real data G(z) = generator output from noise z D(G(z)) = discriminator output for fake data Properties:\nTwo-player minimax game Generator tries to fool discriminator Discriminator tries to distinguish real from fake Training can be unstable (vanishing gradients) When to use:\nOriginal GAN formulation Understanding GAN fundamentals Not recommended for production (use improved versions) Issues:\nVanishing gradients when discriminator is too good Mode collapse (generator produces limited diversity) Training instability Least Squares GAN (LSGAN) Formula:\nGenerator Loss:\nL_G = (1/2) * E[(D(G(z)) - 1)¬≤] Discriminator Loss:\nL_D = (1/2) * E[(D(x) - 1)¬≤] + (1/2) * E[D(G(z))¬≤] Properties:\nUses L2 loss instead of cross-entropy More stable gradients Penalizes samples far from decision boundary Reduces mode collapse compared to original GAN When to use:\nNeed more stable GAN training Want to avoid vanishing gradients Image generation tasks Advantages:\nSmoother loss landscape Better gradient flow More stable training Wasserstein Loss (WGAN) Formula:\nGenerator Loss:\nL_G = -E[D(G(z))] Discriminator Loss (Critic):\nL_D = E[D(G(z))] - E[D(x)] With gradient penalty (WGAN-GP):\nL_D = E[D(G(z))] - E[D(x)] + Œª * E[(||‚àáD(xÃÇ)|| - 1)¬≤] Where:\nxÃÇ = random interpolation between real and fake samples Œª = gradient penalty weight (typically 10) Discriminator (called \u0026ldquo;critic\u0026rdquo;) must be 1-Lipschitz Properties:\nMeasures Wasserstein-1 distance (Earth Mover\u0026rsquo;s Distance) Provides meaningful training signal even when discriminator is optimal More stable than original GAN Requires weight clipping (WGAN) or gradient penalty (WGAN-GP) When to use:\nNeed stable GAN training Want to avoid mode collapse Image generation, style transfer When discriminator accuracy is not meaningful Advantages:\nStable gradients throughout training Meaningful loss value (correlates with sample quality) Less mode collapse Better convergence properties üü® 7. Reinforcement Learning Losses Policy Gradient Loss (REINFORCE) Formula:\nL_PG = -E[log(œÄ(a|s)) * A(s,a)] Where:\nœÄ(a|s) = policy probability of action a given state s A(s,a) = Q(s,a) - V(s) = advantage function Q(s,a) = action-value function V(s) = state-value function (baseline) Properties:\nMaximizes expected return Uses advantage to reduce variance On-policy algorithm High variance (requires many samples) When to use:\nPolicy gradient methods (REINFORCE, Actor-Critic) Continuous/discrete action spaces Need to learn stochastic policies Note: Baseline (V(s)) reduces variance without introducing bias.\nValue Function Loss Formula:\nL_V = E[(V(s) - R)^2] Where:\nV(s) = predicted state value R = actual return (discounted sum of rewards) For TD learning:\nL_V = E[(V(s) - (r + Œ≥ * V(s\u0026#39;)))^2] Where:\nr = immediate reward Œ≥ = discount factor s' = next state Properties:\nRegression loss (typically MSE) Learns to predict expected returns Used in Actor-Critic, DQN, A3C Provides baseline for policy gradient When to use:\nValue-based methods (DQN) Actor-Critic architectures Need to estimate state/action values Baseline estimation for policy gradients PPO Loss (Clipped) Formula:\nL_PPO = E[min(r(Œ∏) * A, clip(r(Œ∏), 1-Œµ, 1+Œµ) * A)] Where:\nr(Œ∏) = œÄ_Œ∏(a|s) / œÄ_Œ∏_old(a|s) (importance sampling ratio) A = advantage estimate Œµ = clipping parameter (typically 0.1-0.3) Properties:\nClips policy updates to prevent large changes Prevents policy from moving too far from old policy More stable than vanilla policy gradient State-of-the-art for many RL tasks When to use:\nNeed stable policy gradient training Continuous control tasks When sample efficiency matters Modern RL applications (robotics, games) Advantages:\nPrevents destructive policy updates Better sample efficiency Easier to tune than TRPO Works well with function approximation Typical hyperparameters:\nŒµ = 0.2 (clipping range) Learning rate = 3e-4 Multiple epochs per batch (typically 4-10) üü© 8. Autoencoder \u0026amp; Reconstruction Losses 8.1 Reconstruction Loss Formula:\nMSE Reconstruction:\nL_recon_MSE = (1/n) * Œ£(x_i - xÃÇ_i)¬≤ L1 Reconstruction:\nL_recon_L1 = (1/n) * Œ£|x_i - xÃÇ_i| Where:\nx_i = original input xÃÇ_i = reconstructed output Properties:\nMeasures how well the model reconstructs input MSE: smooth, penalizes large errors quadratically L1: robust to outliers, linear penalty Used in standard autoencoders, denoising autoencoders When to use:\nMSE: When input is continuous, normally distributed errors L1: When input has outliers, need robustness Standard autoencoders, denoising tasks Compression, dimensionality reduction Note: For images, can also use perceptual losses (VGG features) instead of pixel-wise losses.\n8.2 KL Divergence (VAE) Formula:\nVAE Total Loss:\nL_VAE = L_recon + Œ≤ * L_KL Where:\nL_KL = KL(q(z|x) || p(z)) For Gaussian prior and posterior:\nL_KL = (1/2) * Œ£(œÉ¬≤ + Œº¬≤ - 1 - log(œÉ¬≤)) Where:\nq(z|x) = encoder output (posterior): N(Œº, œÉ¬≤) p(z) = prior: N(0, I) Œº, œÉ = encoder outputs (mean and log-variance) Œ≤ = regularization strength (Œ≤-VAE, typically 1.0) Properties:\nRegularizes latent space to match prior distribution Encourages smooth, continuous latent space Prevents posterior collapse (encoder ignores input) Enables sampling and interpolation in latent space When to use:\nVariational Autoencoders (VAEs) Need to sample from latent space Want smooth latent representations Generative modeling with continuous latent variables Œ≤-VAE:\nŒ≤ \u0026gt; 1: stronger regularization, better disentanglement Œ≤ \u0026lt; 1: weaker regularization, better reconstruction Œ≤ = 1: standard VAE Common Issues:\nPosterior collapse: Œ≤ too large ‚Üí encoder ignores input Blurry reconstructions: Common with MSE reconstruction KL vanishing: Œ≤ too small ‚Üí latent space not regularized üìä Summary: Quick Reference Guide Loss Function Selection by Task Task Type Recommended Loss Key Considerations Regression MSE, MAE, Huber MSE for normal errors, MAE for outliers, Huber for balance Binary Classification Binary Cross-Entropy Use with sigmoid activation Multi-class Classification Categorical Cross-Entropy Use with softmax activation Imbalanced Classification Focal Loss Adjust Œ≥ and Œ± for class imbalance Image Segmentation Dice Loss, IoU Loss, BCE+Dice Dice/IoU for overlap, combined for stability Language Modeling Cross-Entropy (token-level) Standard for next-token prediction Similarity Learning Contrastive Loss, Triplet Loss Contrastive for pairs, Triplet for relative distances GAN Training Wasserstein Loss (WGAN-GP) Most stable, use gradient penalty VAE Reconstruction + KL Divergence Balance Œ≤ for reconstruction vs regularization Reinforcement Learning PPO Loss Clipped for stability Key Properties to Consider Differentiability: Most losses need to be differentiable for backpropagation Robustness: Some losses (MAE, Huber) are more robust to outliers Gradient Behavior: Smooth gradients (MSE) vs non-smooth (MAE at zero) Scale Sensitivity: Some losses are scale-dependent, others are scale-invariant Class Imbalance: Focal Loss, Dice Loss handle imbalance better Direct Optimization: Dice/IoU directly optimize evaluation metrics Common Patterns Combined Losses: Often combine multiple losses (e.g., BCE + Dice, Reconstruction + KL) Weighted Losses: Use weighting factors (Œ±, Œ≤) to balance different objectives Adaptive Losses: Some losses adapt during training (e.g., curriculum learning) Task-Specific: Choose loss that matches your evaluation metric when possible Best Practices Match Loss to Metric: If evaluating with IoU, use IoU loss Handle Edge Cases: Add epsilon to logarithms, handle division by zero Normalize Appropriately: Consider batch normalization, loss normalization Monitor Training: Watch for vanishing/exploding gradients Experiment: Try different losses and combinations for your specific problem ","permalink":"https://rapternmn.github.io/posts/loss_functions/","summary":"\u003ch1 id=\"-1-what-is-a-loss-function-high-level-overview\"\u003eüß† 1. What is a Loss Function? (High-Level Overview)\u003c/h1\u003e\n\u003cp\u003eA loss function measures the difference between the model\u0026rsquo;s prediction and the ground truth.\u003c/p\u003e\n\u003cp\u003eIt answers:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e‚û° How wrong is the model?\u003c/li\u003e\n\u003cli\u003e‚û° In which direction should the parameters change?\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eDuring training, optimization tries to minimize the loss by updating weights (using gradients).\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eLoss ‚Üí Gradient ‚Üí Weight Update ‚Üí Better Predictions.\u003c/strong\u003e\u003c/p\u003e\n\u003chr\u003e\n\u003ch2 id=\"table-of-contents\"\u003eTable of Contents\u003c/h2\u003e\n\u003col\u003e\n\u003cli\u003e\u003ca href=\"#-1-what-is-a-loss-function-high-level-overview\"\u003eWhat is a Loss Function?\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#-2-why-do-we-need-loss-functions\"\u003eWhy do we need Loss Functions?\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#-how-are-loss-functions-used-in-training\"\u003eHow Are Loss Functions Used in Training?\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#-3-types-of-loss-functions-in-deep-learning\"\u003eTypes of Loss Functions in Deep Learning\u003c/a\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"#-1-regression-losses\"\u003eRegression Losses\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#-2-classification-losses\"\u003eClassification Losses\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#-3-ranking--metric-learning-losses\"\u003eRanking / Metric Learning Losses\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#-4-sequence-modeling-losses\"\u003eSequence Modeling Losses\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#-5-image-losses\"\u003eImage Losses\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#-6-generative--autoencoder-losses\"\u003eGenerative / Autoencoder Losses\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#-7-reinforcement-learning-losses\"\u003eReinforcement Learning Losses\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#-8-autoencoder--reconstruction-losses\"\u003eAutoencoder \u0026amp; Reconstruction Losses\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"#-summary-quick-reference-guide\"\u003eSummary: Quick Reference Guide\u003c/a\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003chr\u003e\n\u003ch2 id=\"-2-why-do-we-need-loss-functions\"\u003eüå≥ 2. Why do we need Loss Functions?\u003c/h2\u003e\n\u003ch3 id=\"-they-quantify-model-error\"\u003e‚úîÔ∏è They quantify model error\u003c/h3\u003e\n\u003cp\u003eWithout a loss, we cannot measure how good or bad the model is.\u003c/p\u003e","title":"Loss_functions"}]