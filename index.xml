<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/"><channel><title>Rapternmn Blog</title><link>https://rapternmn.github.io/</link><description>Recent content on Rapternmn Blog</description><generator>Hugo -- 0.152.2</generator><language>en-us</language><lastBuildDate>Fri, 21 Nov 2025 19:56:11 +0530</lastBuildDate><atom:link href="https://rapternmn.github.io/index.xml" rel="self" type="application/rss+xml"/><item><title>Loss_functions</title><link>https://rapternmn.github.io/posts/loss_functions/</link><pubDate>Fri, 21 Nov 2025 19:56:11 +0530</pubDate><guid>https://rapternmn.github.io/posts/loss_functions/</guid><description>&lt;h1 id="-1-what-is-a-loss-function-high-level-overview"&gt;üß† 1. What is a Loss Function? (High-Level Overview)&lt;/h1&gt;
&lt;p&gt;A loss function measures the difference between the model&amp;rsquo;s prediction and the ground truth.&lt;/p&gt;
&lt;p&gt;It answers:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;‚û° How wrong is the model?&lt;/li&gt;
&lt;li&gt;‚û° In which direction should the parameters change?&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;During training, optimization tries to minimize the loss by updating weights (using gradients).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Loss ‚Üí Gradient ‚Üí Weight Update ‚Üí Better Predictions.&lt;/strong&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id="table-of-contents"&gt;Table of Contents&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href="#-1-what-is-a-loss-function-high-level-overview"&gt;What is a Loss Function?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#-2-why-do-we-need-loss-functions"&gt;Why do we need Loss Functions?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#-how-are-loss-functions-used-in-training"&gt;How Are Loss Functions Used in Training?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#-3-types-of-loss-functions-in-deep-learning"&gt;Types of Loss Functions in Deep Learning&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#-1-regression-losses"&gt;Regression Losses&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#-2-classification-losses"&gt;Classification Losses&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#-3-ranking--metric-learning-losses"&gt;Ranking / Metric Learning Losses&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#-4-sequence-modeling-losses"&gt;Sequence Modeling Losses&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#-5-image-losses"&gt;Image Losses&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#-6-generative--autoencoder-losses"&gt;Generative / Autoencoder Losses&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#-7-reinforcement-learning-losses"&gt;Reinforcement Learning Losses&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#-8-autoencoder--reconstruction-losses"&gt;Autoencoder &amp;amp; Reconstruction Losses&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="#-summary-quick-reference-guide"&gt;Summary: Quick Reference Guide&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;hr&gt;
&lt;h2 id="-2-why-do-we-need-loss-functions"&gt;üå≥ 2. Why do we need Loss Functions?&lt;/h2&gt;
&lt;h3 id="-they-quantify-model-error"&gt;‚úîÔ∏è They quantify model error&lt;/h3&gt;
&lt;p&gt;Without a loss, we cannot measure how good or bad the model is.&lt;/p&gt;</description></item></channel></rss>