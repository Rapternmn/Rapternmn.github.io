+++
title = "Recurrent Neural Networks (RNN) & LSTM Architectures"
date = 2025-11-22T11:00:00+05:30
draft = false
description = "Comprehensive guide to Recurrent Neural Networks covering vanilla RNN, LSTM, GRU, bidirectional RNNs, sequence-to-sequence architectures, and their applications in NLP, time series, and sequential data processing."
+++

# ğŸ”„ Recurrent Neural Networks (RNN) & LSTM Architectures

Recurrent Neural Networks (RNNs) are a class of neural networks designed to process sequential data by maintaining hidden states that capture information from previous time steps. This document covers fundamental RNN architectures, their mathematical formulations, variants, and applications.

**Key Concepts:**
- Sequential data processing
- Hidden state propagation
- Vanishing/exploding gradient problems
- Gating mechanisms (LSTM, GRU)
- Bidirectional and deep architectures

---

## ğŸŒ³ 1. Basic RNN (Vanilla RNN)

The basic RNN is the simplest recurrent architecture that processes sequences by maintaining a hidden state that gets updated at each time step.

### Mathematical Formulation

**Forward Pass:**

At each time step `t`:
```
hâ‚œ = tanh(Wâ‚•â‚• Â· hâ‚œâ‚‹â‚ + Wâ‚“â‚• Â· xâ‚œ + bâ‚•)
yâ‚œ = Wâ‚•áµ§ Â· hâ‚œ + báµ§
```

Where:
- `hâ‚œ` = hidden state at time `t` (shape: `[hidden_size]`)
- `hâ‚œâ‚‹â‚` = hidden state at previous time step
- `xâ‚œ` = input at time `t` (shape: `[input_size]`)
- `yâ‚œ` = output at time `t` (shape: `[output_size]`)
- `Wâ‚•â‚•` = weight matrix for hidden-to-hidden (shape: `[hidden_size Ã— hidden_size]`)
- `Wâ‚“â‚•` = weight matrix for input-to-hidden (shape: `[hidden_size Ã— input_size]`)
- `Wâ‚•áµ§` = weight matrix for hidden-to-output (shape: `[output_size Ã— hidden_size]`)
- `bâ‚•`, `báµ§` = bias vectors
- `tanh` = hyperbolic tangent activation function

**Initial Hidden State:**
```
hâ‚€ = 0  (typically initialized to zeros)
```

**Vectorized Form (for batch of size B):**
```
Hâ‚œ = tanh(Hâ‚œâ‚‹â‚ Â· Wâ‚•â‚•áµ€ + Xâ‚œ Â· Wâ‚“â‚•áµ€ + bâ‚•)
Yâ‚œ = Hâ‚œ Â· Wâ‚•áµ§áµ€ + báµ§
```

Where:
- `Hâ‚œ` = `[B Ã— hidden_size]`
- `Xâ‚œ` = `[B Ã— input_size]`
- `Yâ‚œ` = `[B Ã— output_size]`

### Loss Function

**For Sequence-to-Sequence Tasks:**
```
L = (1/T) * Î£â‚œâ‚Œâ‚áµ€ Lâ‚œ(yâ‚œ, Å·â‚œ)
```

Where `Lâ‚œ` is the loss at time step `t` (e.g., cross-entropy for classification, MSE for regression).

**For Sequence-to-One Tasks:**
```
L = L(yâ‚œ, Å·â‚œ)  (only final output)
```

### Backpropagation Through Time (BPTT)

**Gradient w.r.t. Hidden State:**
```
âˆ‚L/âˆ‚hâ‚œ = âˆ‚L/âˆ‚yâ‚œ Â· Wâ‚•áµ§ + âˆ‚L/âˆ‚hâ‚œâ‚Šâ‚ Â· Wâ‚•â‚• Â· (1 - tanhÂ²(zâ‚œâ‚Šâ‚))
```

Where `zâ‚œ = Wâ‚•â‚• Â· hâ‚œâ‚‹â‚ + Wâ‚“â‚• Â· xâ‚œ + bâ‚•`.

**Gradient w.r.t. Weights:**
```
âˆ‚L/âˆ‚Wâ‚•â‚• = Î£â‚œâ‚Œâ‚áµ€ âˆ‚L/âˆ‚hâ‚œ Â· (1 - tanhÂ²(zâ‚œ)) Â· hâ‚œâ‚‹â‚áµ€
âˆ‚L/âˆ‚Wâ‚“â‚• = Î£â‚œâ‚Œâ‚áµ€ âˆ‚L/âˆ‚hâ‚œ Â· (1 - tanhÂ²(zâ‚œ)) Â· xâ‚œáµ€
âˆ‚L/âˆ‚Wâ‚•áµ§ = Î£â‚œâ‚Œâ‚áµ€ âˆ‚L/âˆ‚yâ‚œ Â· hâ‚œáµ€
```

### Properties

**Advantages:**
- **Sequential processing:** Can handle variable-length sequences
- **Parameter sharing:** Same weights across all time steps
- **Memory:** Hidden state captures information from previous steps
- **Flexible:** Can be used for sequence-to-sequence, sequence-to-one, one-to-sequence tasks

**Limitations:**
- **Vanishing gradients:** Gradients decay exponentially over time steps
- **Exploding gradients:** Gradients can grow exponentially (less common)
- **Short-term memory:** Struggles with long-range dependencies
- **Computational bottleneck:** Sequential processing prevents parallelization

### Vanishing Gradient Problem

**Root Cause:**
The gradient flows through time via repeated multiplication by `Wâ‚•â‚•`:
```
âˆ‚L/âˆ‚hâ‚€ = âˆ‚L/âˆ‚hâ‚œ Â· Wâ‚•â‚•áµ€ Â· Wâ‚•â‚•áµ€ Â· ... Â· Wâ‚•â‚•áµ€ Â· (1 - tanhÂ²(zâ‚)) Â· ... Â· (1 - tanhÂ²(zâ‚œ))
```

If eigenvalues of `Wâ‚•â‚•` are < 1, gradients vanish. If > 1, gradients explode.

**Impact:**
- Early time steps receive very small gradients
- Network cannot learn long-range dependencies
- Training becomes very slow or ineffective

---

## ğŸŸ¦ 2. Long Short-Term Memory (LSTM)

LSTM was designed to solve the vanishing gradient problem by introducing gating mechanisms that allow the network to selectively remember or forget information.

### Architecture Overview

An LSTM cell has three gates:
1. **Forget Gate:** Decides what information to discard from cell state
2. **Input Gate:** Decides what new information to store in cell state
3. **Output Gate:** Decides what parts of cell state to output

### Mathematical Formulation

**Cell Components:**

**Forget Gate:**
```
fâ‚œ = Ïƒ(Wâ‚“f Â· xâ‚œ + Wâ‚•f Â· hâ‚œâ‚‹â‚ + bf)
```

**Input Gate:**
```
iâ‚œ = Ïƒ(Wâ‚“áµ¢ Â· xâ‚œ + Wâ‚•áµ¢ Â· hâ‚œâ‚‹â‚ + bi)
CÌƒâ‚œ = tanh(Wâ‚“C Â· xâ‚œ + Wâ‚•C Â· hâ‚œâ‚‹â‚ + bC)
```

**Cell State Update:**
```
Câ‚œ = fâ‚œ âŠ™ Câ‚œâ‚‹â‚ + iâ‚œ âŠ™ CÌƒâ‚œ
```

**Output Gate:**
```
oâ‚œ = Ïƒ(Wâ‚“â‚’ Â· xâ‚œ + Wâ‚•â‚’ Â· hâ‚œâ‚‹â‚ + bo)
hâ‚œ = oâ‚œ âŠ™ tanh(Câ‚œ)
```

Where:
- `Ïƒ` = sigmoid activation function
- `âŠ™` = element-wise multiplication (Hadamard product)
- `Câ‚œ` = cell state at time `t`
- `Câ‚œâ‚‹â‚` = cell state at previous time step
- `hâ‚œ` = hidden state (output) at time `t`
- `fâ‚œ`, `iâ‚œ`, `oâ‚œ` = forget, input, output gates (all in range [0, 1])
- `CÌƒâ‚œ` = candidate cell state values

**Initial States:**
```
hâ‚€ = 0
Câ‚€ = 0
```

### Parameter Count

For an LSTM with `input_size = d` and `hidden_size = h`:
- Forget gate: `4hÂ² + 4hd + 4h` parameters
- Input gate: `4hÂ² + 4hd + 4h` parameters
- Cell state: `4hÂ² + 4hd + 4h` parameters
- Output gate: `4hÂ² + 4hd + 4h` parameters

**Total:** `16hÂ² + 16hd + 16h = 16h(h + d + 1)` parameters

### Why LSTM Solves Vanishing Gradients

**Key Insight:** The cell state `Câ‚œ` has a linear path through time:
```
Câ‚œ = fâ‚œ âŠ™ Câ‚œâ‚‹â‚ + iâ‚œ âŠ™ CÌƒâ‚œ
```

The gradient flows through this path with minimal decay:
```
âˆ‚Câ‚œ/âˆ‚Câ‚œâ‚‹â‚ = fâ‚œ  (element-wise)
```

Since `fâ‚œ` is learned and can be close to 1, gradients can flow through many time steps without vanishing.

### Properties

**Advantages:**
- **Long-term memory:** Can remember information for many time steps
- **Selective memory:** Gates allow fine-grained control over what to remember/forget
- **Gradient flow:** Better gradient propagation than vanilla RNN
- **Flexible:** Works well for various sequence tasks

**Limitations:**
- **Computational cost:** More parameters and operations than vanilla RNN
- **Complexity:** More hyperparameters to tune
- **Still sequential:** Cannot parallelize across time steps
- **Memory intensive:** Stores both hidden state and cell state

### Variants

#### Peephole Connections
Allows gates to see the cell state:
```
fâ‚œ = Ïƒ(Wâ‚“f Â· xâ‚œ + Wâ‚•f Â· hâ‚œâ‚‹â‚ + Wâ‚f Â· Câ‚œâ‚‹â‚ + bf)
```

#### Coupled Input-Forget Gate
Combines input and forget gates:
```
Câ‚œ = (1 - iâ‚œ) âŠ™ Câ‚œâ‚‹â‚ + iâ‚œ âŠ™ CÌƒâ‚œ
```

---

## ğŸŸ© 3. Gated Recurrent Unit (GRU)

GRU is a simplified variant of LSTM that combines the forget and input gates into a single "update gate" and merges the cell state and hidden state.

### Mathematical Formulation

**Update Gate:**
```
zâ‚œ = Ïƒ(Wâ‚“z Â· xâ‚œ + Wâ‚•z Â· hâ‚œâ‚‹â‚ + bz)
```

**Reset Gate:**
```
râ‚œ = Ïƒ(Wâ‚“r Â· xâ‚œ + Wâ‚•r Â· hâ‚œâ‚‹â‚ + br)
```

**Candidate Hidden State:**
```
hÌƒâ‚œ = tanh(Wâ‚“h Â· xâ‚œ + Wâ‚•h Â· (râ‚œ âŠ™ hâ‚œâ‚‹â‚) + bh)
```

**Hidden State Update:**
```
hâ‚œ = (1 - zâ‚œ) âŠ™ hâ‚œâ‚‹â‚ + zâ‚œ âŠ™ hÌƒâ‚œ
```

Where:
- `zâ‚œ` = update gate (controls how much of previous state to keep)
- `râ‚œ` = reset gate (controls how much of previous state to forget)
- `hÌƒâ‚œ` = candidate hidden state
- `hâ‚œ` = final hidden state

**Initial State:**
```
hâ‚€ = 0
```

### Parameter Count

For a GRU with `input_size = d` and `hidden_size = h`:
- Update gate: `hÂ² + hd + h` parameters
- Reset gate: `hÂ² + hd + h` parameters
- Candidate state: `hÂ² + hd + h` parameters

**Total:** `3hÂ² + 3hd + 3h = 3h(h + d + 1)` parameters

### Properties

**Advantages:**
- **Simpler than LSTM:** Fewer parameters (3 gates vs 4 in LSTM)
- **Faster training:** Less computation per time step
- **Often comparable performance:** Works as well as LSTM on many tasks
- **Better gradient flow:** Still solves vanishing gradient problem

**Limitations:**
- **Less expressive:** May struggle with very long sequences compared to LSTM
- **Still sequential:** Cannot parallelize across time steps
- **Memory:** Still needs to store hidden state

### LSTM vs GRU

| Aspect | LSTM | GRU |
|--------|------|-----|
| **Gates** | 3 (forget, input, output) | 2 (update, reset) |
| **States** | Hidden + Cell | Hidden only |
| **Parameters** | ~4Ã— more | Fewer |
| **Complexity** | Higher | Lower |
| **Performance** | Better on long sequences | Often comparable |
| **Training Speed** | Slower | Faster |

**When to use GRU:**
- Limited computational resources
- Shorter sequences
- When LSTM performance is similar

**When to use LSTM:**
- Very long sequences
- Complex long-range dependencies
- When maximum performance is needed

---

## ğŸ”€ 4. Bidirectional RNN (BiRNN)

Bidirectional RNNs process sequences in both forward and backward directions, allowing the network to use information from both past and future contexts.

### Architecture

**Forward Pass:**
```
hâ‚œâ†’ = f(Wâ‚“â‚•â†’ Â· xâ‚œ + Wâ‚•â‚•â†’ Â· hâ‚œâ‚‹â‚â†’ + bâ†’)
```

**Backward Pass:**
```
hâ‚œâ† = f(Wâ‚“â‚•â† Â· xâ‚œ + Wâ‚•â‚•â† Â· hâ‚œâ‚Šâ‚â† + bâ†)
```

**Combined Output:**
```
hâ‚œ = [hâ‚œâ†’; hâ‚œâ†]  (concatenation)
yâ‚œ = Wâ‚•áµ§ Â· hâ‚œ + báµ§
```

Where:
- `hâ‚œâ†’` = forward hidden state
- `hâ‚œâ†` = backward hidden state
- `[;]` = concatenation operator

### Variants

#### Bidirectional LSTM (BiLSTM)
Uses LSTM cells in both directions:
```
hâ‚œâ†’ = LSTMâ†’(xâ‚œ, hâ‚œâ‚‹â‚â†’, Câ‚œâ‚‹â‚â†’)
hâ‚œâ† = LSTMâ†(xâ‚œ, hâ‚œâ‚Šâ‚â†, Câ‚œâ‚Šâ‚â†)
hâ‚œ = [hâ‚œâ†’; hâ‚œâ†]
```

#### Bidirectional GRU (BiGRU)
Uses GRU cells in both directions:
```
hâ‚œâ†’ = GRUâ†’(xâ‚œ, hâ‚œâ‚‹â‚â†’)
hâ‚œâ† = GRUâ†(xâ‚œ, hâ‚œâ‚Šâ‚â†)
hâ‚œ = [hâ‚œâ†’; hâ‚œâ†]
```

### Properties

**Advantages:**
- **Context awareness:** Can use both past and future information
- **Better representations:** Often produces richer feature representations
- **Useful for:** Named entity recognition, sentiment analysis, machine translation

**Limitations:**
- **Requires full sequence:** Cannot be used in online/streaming scenarios
- **More parameters:** Approximately 2Ã— parameters of unidirectional RNN
- **Slower inference:** Must process entire sequence before output

### Applications

- **Named Entity Recognition (NER):** Context from both sides helps identify entities
- **Sentiment Analysis:** Future words can clarify sentiment of current words
- **Machine Translation:** Better understanding of source sentence structure

---

## ğŸ“š 5. Deep RNN (Stacked RNN)

Deep RNNs stack multiple RNN layers on top of each other, allowing the network to learn hierarchical representations of sequential data.

### Architecture

**Multi-Layer RNN:**
```
hâ‚œâ½Â¹â¾ = RNNâ‚(xâ‚œ, hâ‚œâ‚‹â‚â½Â¹â¾)
hâ‚œâ½Â²â¾ = RNNâ‚‚(hâ‚œâ½Â¹â¾, hâ‚œâ‚‹â‚â½Â²â¾)
...
hâ‚œâ½á´¸â¾ = RNNâ‚—(hâ‚œâ½á´¸â»Â¹â¾, hâ‚œâ‚‹â‚â½á´¸â¾)
yâ‚œ = W Â· hâ‚œâ½á´¸â¾ + b
```

Where:
- `L` = number of layers
- `hâ‚œâ½Ë¡â¾` = hidden state of layer `l` at time `t`
- Each layer can be vanilla RNN, LSTM, or GRU

### Properties

**Advantages:**
- **Hierarchical features:** Lower layers capture local patterns, higher layers capture abstract patterns
- **Increased capacity:** More parameters allow modeling complex sequences
- **Better representations:** Often improves performance on complex tasks

**Limitations:**
- **Training difficulty:** Deeper networks are harder to train
- **Computational cost:** More layers = more computation
- **Overfitting risk:** More parameters can lead to overfitting
- **Gradient issues:** Still susceptible to vanishing gradients (though LSTM/GRU help)

### Best Practices

- **Layer normalization:** Apply layer norm between RNN layers
- **Residual connections:** Add skip connections to help gradient flow
- **Dropout:** Apply dropout between layers (not in recurrent connections)
- **Gradual depth:** Start with 2-3 layers, increase if needed

---

## ğŸ”„ 6. Sequence-to-Sequence (Seq2Seq) Architecture

Seq2Seq models use an encoder-decoder architecture to map variable-length input sequences to variable-length output sequences.

### Architecture

**Encoder:**
```
hâ‚œáµ‰â¿á¶œ = RNN(xâ‚œ, hâ‚œâ‚‹â‚áµ‰â¿á¶œ)
c = hâ‚œáµ‰â¿á¶œ  (context vector from final hidden state)
```

**Decoder:**
```
hâ‚œáµˆáµ‰á¶œ = RNN(yâ‚œâ‚‹â‚, hâ‚œâ‚‹â‚áµˆáµ‰á¶œ, c)
yâ‚œ = softmax(W Â· hâ‚œáµˆáµ‰á¶œ + b)
```

Where:
- Encoder processes input sequence `xâ‚, ..., xâ‚œ`
- Context vector `c` summarizes entire input
- Decoder generates output sequence `yâ‚, ..., yâ‚œ'` one token at a time

### Attention Mechanism

**Problem with Basic Seq2Seq:**
- Single context vector `c` must encode entire input sequence
- Information bottleneck for long sequences
- All input positions treated equally

**Solution: Attention**
```
Attention weights: Î±â‚œáµ¢ = softmax(score(hâ‚œáµˆáµ‰á¶œ, háµ¢áµ‰â¿á¶œ))
Context vector: câ‚œ = Î£áµ¢ Î±â‚œáµ¢ Â· háµ¢áµ‰â¿á¶œ
```

**Attention Variants:**

**Dot Product Attention:**
```
score(hâ‚œáµˆáµ‰á¶œ, háµ¢áµ‰â¿á¶œ) = hâ‚œáµˆáµ‰á¶œáµ€ Â· háµ¢áµ‰â¿á¶œ
```

**General Attention:**
```
score(hâ‚œáµˆáµ‰á¶œ, háµ¢áµ‰â¿á¶œ) = hâ‚œáµˆáµ‰á¶œáµ€ Â· Wâ‚ Â· háµ¢áµ‰â¿á¶œ
```

**Additive Attention (Bahdanau):**
```
score(hâ‚œáµˆáµ‰á¶œ, háµ¢áµ‰â¿á¶œ) = váµ€ Â· tanh(Wâ‚ Â· hâ‚œáµˆáµ‰á¶œ + Wâ‚‚ Â· háµ¢áµ‰â¿á¶œ)
```

### Properties

**Advantages:**
- **Variable length:** Handles sequences of different lengths
- **Flexible:** Can be used for translation, summarization, dialogue
- **Attention:** Allows focusing on relevant parts of input

**Limitations:**
- **Sequential decoding:** Cannot parallelize output generation
- **Slow inference:** Must generate tokens one at a time
- **Context limitation:** Fixed-size context vector in basic version

---

## ğŸ¯ 7. Applications and Use Cases

### Natural Language Processing (NLP)
- **Machine Translation:** Seq2Seq with attention
- **Text Generation:** Language modeling with RNN/LSTM
- **Sentiment Analysis:** Classification using BiLSTM
- **Named Entity Recognition:** Sequence labeling with BiLSTM + CRF

### Speech Recognition
- **Speech-to-Text:** Acoustic modeling with LSTM
- **Voice Activity Detection:** Sequence classification

### Time Series Forecasting
- **Stock Price Prediction:** LSTM for financial time series
- **Weather Forecasting:** RNN for temporal patterns
- **Demand Forecasting:** Sequence prediction

### Other Applications
- **Video Analysis:** Frame-by-frame processing
- **Music Generation:** Sequential pattern learning
- **Protein Structure Prediction:** Biological sequence analysis

---

## ğŸ”§ 8. Training Techniques

### Gradient Clipping

**Problem:** Exploding gradients in RNNs

**Solution:** Clip gradients to a maximum norm:
```
if ||g|| > threshold:
    g = g Â· (threshold / ||g||)
```

Where `g` is the gradient vector.

### Truncated BPTT

**Problem:** Full BPTT is computationally expensive for long sequences

**Solution:** Only backpropagate through a fixed window:
```
Backpropagate through last K time steps only
```

### Teacher Forcing

**Problem:** During training, decoder uses its own (potentially wrong) predictions

**Solution:** Use ground truth labels during training:
```
Training: hâ‚œ = RNN(yâ‚œâ‚‹â‚áµ—Ê³áµ˜áµ‰, hâ‚œâ‚‹â‚)
Inference: hâ‚œ = RNN(yâ‚œâ‚‹â‚áµ–Ê³áµ‰áµˆ, hâ‚œâ‚‹â‚)
```

### Scheduled Sampling

Gradually transition from teacher forcing to using predictions:
```
Use ground truth with probability p, prediction with (1-p)
p decreases during training
```

---

## ğŸ“Š 9. Comparison Summary

| Architecture | Parameters | Memory | Training Speed | Long Sequences | Use Case |
|--------------|------------|--------|----------------|----------------|----------|
| **Vanilla RNN** | Low | Low | Fast | Poor | Simple sequences |
| **LSTM** | High | High | Slow | Excellent | Long sequences, complex dependencies |
| **GRU** | Medium | Medium | Medium | Good | Balanced performance/speed |
| **BiRNN** | 2Ã— | 2Ã— | 2Ã— slower | Depends on cell | Context-aware tasks |
| **Deep RNN** | LÃ— | LÃ— | LÃ— slower | Better | Complex hierarchical patterns |
| **Seq2Seq** | Very High | Very High | Very Slow | Excellent | Translation, summarization |

---

## ğŸ“ 10. Key Takeaways

1. **RNNs are designed for sequential data** but suffer from vanishing gradients
2. **LSTM solves vanishing gradients** through gating mechanisms and cell state
3. **GRU is a simpler alternative** to LSTM with comparable performance
4. **Bidirectional RNNs** use both past and future context
5. **Deep RNNs** learn hierarchical representations
6. **Seq2Seq with attention** enables variable-length sequence mapping
7. **Training techniques** like gradient clipping and teacher forcing are essential
8. **Choose architecture based on:** sequence length, computational budget, task requirements

---

## ğŸ“š References & Further Reading

- **Original LSTM Paper:** Hochreiter & Schmidhuber (1997) - "Long Short-Term Memory"
- **GRU Paper:** Cho et al. (2014) - "Learning Phrase Representations using RNN Encoder-Decoder"
- **Attention Mechanism:** Bahdanau et al. (2014) - "Neural Machine Translation by Jointly Learning to Align and Translate"
- **BPTT:** Werbos (1990) - "Backpropagation Through Time"
- **Modern Applications:** See Transformer architecture for attention-only models

---

*This document covers fundamental RNN architectures. For attention-only models (Transformers), see separate documentation.*

