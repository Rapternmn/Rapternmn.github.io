+++
title = "Deep Neural Networks (DNN): Fundamentals"
date = 2025-11-22T11:00:00+05:30
draft = false
description = "Comprehensive guide to Deep Neural Networks covering feedforward propagation, backpropagation, loss functions, optimization algorithms (SGD, Adam), regularization techniques (dropout, batch normalization), and mathematical foundations."
+++

# ğŸ§  Deep Neural Networks (DNN): Fundamentals

Deep Neural Networks (DNNs) are multi-layer neural networks that can learn complex, hierarchical representations from data. This document covers fundamental DNN algorithms, their mathematical formulations, loss functions, gradient calculations, and training procedures.

**Key Concepts:**
- Feedforward propagation
- Backpropagation algorithm
- Loss functions and optimization
- Gradient computation
- Regularization techniques

---

## ğŸŒ³ 1. Linear Regression (Foundation)

Linear Regression is the simplest form of neural network (a single neuron with linear activation).

### Mathematical Formulation

**Model:**
```
Å· = wâ‚xâ‚ + wâ‚‚xâ‚‚ + ... + wâ‚™xâ‚™ + b = wáµ€x + b
```

Where:
- `w = [wâ‚, wâ‚‚, ..., wâ‚™]áµ€` = weight vector
- `x = [xâ‚, xâ‚‚, ..., xâ‚™]áµ€` = input features
- `b` = bias term
- `Å·` = predicted output

**Matrix Form (for batch of m samples):**
```
Å¶ = XW + b
```

Where:
- `X` = (m Ã— n) input matrix
- `W` = (n Ã— 1) weight vector
- `Å¶` = (m Ã— 1) predictions

### Loss Function: Mean Squared Error (MSE)

**Formula:**
```
L = (1/2m) * Î£(Å·áµ¢ - yáµ¢)Â²
```

**Vectorized Form:**
```
L = (1/2m) * ||Å¶ - Y||Â²
```

### Gradient Calculation

**Gradient w.r.t. weights:**
```
âˆ‚L/âˆ‚wâ±¼ = (1/m) * Î£(Å·áµ¢ - yáµ¢) * xáµ¢â±¼
```

**Vectorized:**
```
âˆ‡w L = (1/m) * Xáµ€(Å¶ - Y)
```

**Gradient w.r.t. bias:**
```
âˆ‚L/âˆ‚b = (1/m) * Î£(Å·áµ¢ - yáµ¢)
```

**Vectorized:**
```
âˆ‡b L = (1/m) * Î£(Å¶ - Y)
```

### Training Algorithm

**Gradient Descent Update:**
```
w := w - Î± * âˆ‡w L
b := b - Î± * âˆ‡b L
```

Where `Î±` is the learning rate.

### Properties

- **Convex optimization:** Guaranteed global minimum
- **Closed-form solution:** Can be solved analytically using normal equation
- **Fast training:** Simple gradient computation
- **Interpretable:** Weights show feature importance

### Limitations

- **Linear relationships only:** Cannot model non-linear patterns
- **Assumes linearity:** May fail on complex data

---

## ğŸŸ¦ 2. Logistic Regression

Logistic Regression is a binary classification algorithm using a sigmoid activation function.

### Mathematical Formulation

**Model:**
```
z = wáµ€x + b
Å· = Ïƒ(z) = 1 / (1 + e^(-z))
```

Where `Ïƒ(z)` is the sigmoid function.

**Output Interpretation:**
- `Å·` represents the probability that `y = 1`
- `P(y=1|x) = Å·`
- `P(y=0|x) = 1 - Å·`

### Loss Function: Binary Cross-Entropy

**Formula:**
```
L = -(1/m) * Î£[yáµ¢ * log(Å·áµ¢) + (1-yáµ¢) * log(1-Å·áµ¢)]
```

**For a single sample:**
```
L = -[y * log(Å·) + (1-y) * log(1-Å·)]
```

**Properties:**
- Derived from maximum likelihood estimation
- Penalizes confident wrong predictions heavily
- Well-suited for probability outputs

### Gradient Calculation

**Step 1: Gradient w.r.t. output Å·**
```
âˆ‚L/âˆ‚Å· = -(y/Å·) + (1-y)/(1-Å·) = (Å· - y) / [Å·(1-Å·)]
```

**Step 2: Gradient w.r.t. z (before sigmoid)**
```
âˆ‚L/âˆ‚z = âˆ‚L/âˆ‚Å· * âˆ‚Å·/âˆ‚z
```

Since `âˆ‚Å·/âˆ‚z = Ïƒ(z)(1-Ïƒ(z)) = Å·(1-Å·)`:
```
âˆ‚L/âˆ‚z = (Å· - y) / [Å·(1-Å·)] * Å·(1-Å·) = Å· - y
```

**Step 3: Gradient w.r.t. weights**
```
âˆ‚L/âˆ‚wâ±¼ = âˆ‚L/âˆ‚z * âˆ‚z/âˆ‚wâ±¼ = (Å· - y) * xâ±¼
```

**Vectorized:**
```
âˆ‡w L = (1/m) * Xáµ€(Å¶ - Y)
```

**Step 4: Gradient w.r.t. bias**
```
âˆ‚L/âˆ‚b = âˆ‚L/âˆ‚z * âˆ‚z/âˆ‚b = (Å· - y)
```

**Vectorized:**
```
âˆ‡b L = (1/m) * Î£(Å¶ - Y)
```

### Training Algorithm

**Gradient Descent Update:**
```
w := w - Î± * âˆ‡w L
b := b - Î± * âˆ‡b L
```

**Complete Algorithm:**
1. Initialize weights `w` and bias `b`
2. For each iteration:
   - Forward pass: Compute `Å· = Ïƒ(wáµ€x + b)`
   - Compute loss: `L = -[y*log(Å·) + (1-y)*log(1-Å·)]`
   - Backward pass: Compute gradients
   - Update parameters: `w := w - Î± * âˆ‡w L`, `b := b - Î± * âˆ‡b L`
3. Repeat until convergence

### Decision Boundary

**Classification Rule:**
```
Predict y = 1 if Å· â‰¥ 0.5 (i.e., z â‰¥ 0)
Predict y = 0 if Å· < 0.5 (i.e., z < 0)
```

**Decision Boundary:**
```
wáµ€x + b = 0
```

This is a linear decision boundary (hyperplane).

### Properties

- **Probabilistic output:** Provides probability estimates
- **Interpretable:** Weights indicate feature importance
- **Convex loss:** Guaranteed convergence to global minimum
- **Efficient:** Fast training and prediction

### Limitations

- **Linear decision boundary:** Cannot handle non-linearly separable data
- **Binary classification only:** Requires extension for multi-class

### Multi-class Extension: Softmax Regression

**Model:**
```
zâ±¼ = wâ±¼áµ€x + bâ±¼  (for each class j)
Å·â±¼ = e^(zâ±¼) / Î£â‚– e^(zâ‚–)  (softmax)
```

**Loss Function: Categorical Cross-Entropy**
```
L = -(1/m) * Î£áµ¢ Î£â±¼ yáµ¢â±¼ * log(Å·áµ¢â±¼)
```

**Gradient:**
```
âˆ‚L/âˆ‚zâ±¼ = Å·â±¼ - yâ±¼
```

---

## ğŸŸ© 3. Perceptron

The Perceptron is the simplest neural network unit, a binary classifier with a step activation function.

### Mathematical Formulation

**Model:**
```
z = wáµ€x + b
Å· = {
    1,  if z â‰¥ 0
    0,  if z < 0
}
```

This is a **step function** (Heaviside function).

### Loss Function: Perceptron Loss

**Formula:**
```
L = {
    0,        if y = Å· (correct prediction)
    -(wáµ€x + b) * y,  if y â‰  Å· (misclassification)
}
```

**Alternative Formulation:**
```
L = max(0, -y * (wáµ€x + b))
```

This is similar to the **hinge loss**.

### Gradient Calculation

**When misclassified (y = 1, Å· = 0):**
```
âˆ‚L/âˆ‚wâ±¼ = -y * xâ±¼ = -xâ±¼
âˆ‚L/âˆ‚b = -y = -1
```

**When misclassified (y = 0, Å· = 1):**
```
âˆ‚L/âˆ‚wâ±¼ = -y * xâ±¼ = 0
âˆ‚L/âˆ‚b = -y = 0
```

**When correctly classified:**
```
âˆ‚L/âˆ‚wâ±¼ = 0
âˆ‚L/âˆ‚b = 0
```

### Training Algorithm: Perceptron Learning Rule

**Algorithm:**
1. Initialize weights `w` and bias `b` (typically to zeros or small random values)
2. For each training sample `(x, y)`:
   - Compute prediction: `Å· = step(wáµ€x + b)`
   - If `Å· â‰  y` (misclassified):
     - Update: `w := w + Î± * y * x`
     - Update: `b := b + Î± * y`
3. Repeat until all samples are correctly classified or max iterations reached

**Update Rule:**
```
w := w + Î± * y * x  (if misclassified)
b := b + Î± * y      (if misclassified)
```

### Convergence Theorem

**Perceptron Convergence Theorem:**
- If data is **linearly separable**, the Perceptron algorithm will converge in a finite number of steps
- The number of mistakes is bounded by `(R/Î³)Â²`, where:
  - `R` = maximum norm of training examples
  - `Î³` = margin (distance from decision boundary to closest point)

### Properties

- **Simple and fast:** Very efficient training
- **Online learning:** Can update with each sample
- **Guaranteed convergence:** For linearly separable data

### Limitations

- **Only linearly separable data:** Cannot learn XOR function
- **No probabilistic output:** Only binary predictions
- **May not converge:** If data is not linearly separable

---

## ğŸŸª 4. Multi-Layer Perceptron (MLP)

A Multi-Layer Perceptron is a feedforward neural network with one or more hidden layers.

### Architecture

**Structure:**
```
Input Layer â†’ Hidden Layer(s) â†’ Output Layer
```

**Example (2-layer MLP):**
```
x â†’ [Linear + ReLU] â†’ h â†’ [Linear + Activation] â†’ Å·
```

### Mathematical Formulation

**Forward Pass:**

**Layer 1 (Hidden):**
```
zÂ¹ = WÂ¹x + bÂ¹
h = ÏƒÂ¹(zÂ¹)  (activation function, e.g., ReLU)
```

**Layer 2 (Output):**
```
zÂ² = WÂ²h + bÂ²
Å· = ÏƒÂ²(zÂ²)  (activation function depends on task)
```

**General Form (L layers):**
```
zË¡ = WË¡aË¡â»Â¹ + bË¡
aË¡ = ÏƒË¡(zË¡)
```

Where:
- `aâ° = x` (input)
- `aá´¸ = Å·` (output)
- `WË¡` = weight matrix for layer `l`
- `bË¡` = bias vector for layer `l`
- `ÏƒË¡` = activation function for layer `l`

### Loss Functions

**For Regression:**
```
L = (1/2m) * Î£(Å·áµ¢ - yáµ¢)Â²  (MSE)
```

**For Binary Classification:**
```
L = -(1/m) * Î£[yáµ¢ * log(Å·áµ¢) + (1-yáµ¢) * log(1-Å·áµ¢)]  (BCE)
```

**For Multi-class Classification:**
```
L = -(1/m) * Î£áµ¢ Î£â±¼ yáµ¢â±¼ * log(Å·áµ¢â±¼)  (CCE)
```

### Backpropagation Algorithm

Backpropagation computes gradients using the chain rule of calculus.

#### Algorithm Steps

**1. Forward Pass:**
```
For l = 1 to L:
    zË¡ = WË¡aË¡â»Â¹ + bË¡
    aË¡ = ÏƒË¡(zË¡)
```

**2. Compute Output Error:**
```
Î´á´¸ = âˆ‚L/âˆ‚aá´¸ * Ïƒ'á´¸(zá´¸)
```

**For MSE loss:**
```
Î´á´¸ = (aá´¸ - y) * Ïƒ'á´¸(zá´¸)
```

**For BCE loss (with sigmoid):**
```
Î´á´¸ = aá´¸ - y
```

**For CCE loss (with softmax):**
```
Î´á´¸ = aá´¸ - y
```

**3. Backward Pass (Error Propagation):**
```
For l = L-1 to 1:
    Î´Ë¡ = (WË¡âºÂ¹)áµ€Î´Ë¡âºÂ¹ âŠ™ Ïƒ'Ë¡(zË¡)
```

Where `âŠ™` denotes element-wise multiplication (Hadamard product).

**4. Compute Gradients:**
```
âˆ‚L/âˆ‚WË¡ = Î´Ë¡(aË¡â»Â¹)áµ€
âˆ‚L/âˆ‚bË¡ = Î´Ë¡
```

**5. Update Parameters:**
```
WË¡ := WË¡ - Î± * âˆ‚L/âˆ‚WË¡
bË¡ := bË¡ - Î± * âˆ‚L/âˆ‚bË¡
```

### Detailed Gradient Derivation

**For a 2-layer MLP:**

**Layer 2 (Output) Gradients:**
```
âˆ‚L/âˆ‚WÂ² = (1/m) * Î´Â²(h)áµ€
âˆ‚L/âˆ‚bÂ² = (1/m) * Î£Î´Â²
```

Where `Î´Â² = (Å· - y) * Ïƒ'Â²(zÂ²)` or `Î´Â² = Å· - y` (if using cross-entropy with softmax/sigmoid).

**Layer 1 (Hidden) Gradients:**
```
Î´Â¹ = (WÂ²)áµ€Î´Â² âŠ™ Ïƒ'Â¹(zÂ¹)
âˆ‚L/âˆ‚WÂ¹ = (1/m) * Î´Â¹(x)áµ€
âˆ‚L/âˆ‚bÂ¹ = (1/m) * Î£Î´Â¹
```

### Activation Function Derivatives

**ReLU:**
```
Ïƒ'(z) = {
    1,  if z > 0
    0,  if z â‰¤ 0
}
```

**Sigmoid:**
```
Ïƒ'(z) = Ïƒ(z)(1 - Ïƒ(z))
```

**Tanh:**
```
Ïƒ'(z) = 1 - tanhÂ²(z)
```

**Linear:**
```
Ïƒ'(z) = 1
```

### Training Algorithm

**Complete Training Loop:**
1. Initialize all weights and biases (e.g., Xavier/Glorot or He initialization)
2. For each epoch:
   - For each batch:
     - **Forward Pass:** Compute predictions
     - **Compute Loss:** Calculate loss on batch
     - **Backward Pass:** Compute gradients using backpropagation
     - **Update Parameters:** Apply gradient descent update
3. Repeat until convergence

### Properties

- **Universal Approximation:** Can approximate any continuous function (with sufficient capacity)
- **Non-linear:** Can learn complex decision boundaries
- **Flexible:** Can handle various tasks (regression, classification)

### Limitations

- **Vanishing/Exploding Gradients:** Deep networks can suffer from gradient problems
- **Overfitting:** Can memorize training data
- **Local Minima:** Non-convex optimization
- **Computational Cost:** Training can be slow for large networks

---

## ğŸŸ§ 5. Feedforward Neural Networks

Feedforward Neural Networks are the general class of DNNs where information flows in one direction (input â†’ output).

### Architecture Types

**1. Fully Connected (Dense) Networks:**
- Every neuron in layer `l` connects to every neuron in layer `l+1`
- Most common type of DNN

**2. Deep Networks:**
- Multiple hidden layers (typically 3+)
- Can learn hierarchical features

### Forward Propagation

**General Formula:**
```
aâ° = x
For l = 1 to L:
    zË¡ = WË¡aË¡â»Â¹ + bË¡
    aË¡ = ÏƒË¡(zË¡)
Å· = aá´¸
```

**Vectorized (for batch):**
```
Aâ° = X
For l = 1 to L:
    ZË¡ = AË¡â»Â¹(WË¡)áµ€ + bË¡  (broadcast)
    AË¡ = ÏƒË¡(ZË¡)
Å¶ = Aá´¸
```

### Backward Propagation

**General Formula:**
```
Î´á´¸ = âˆ‡aá´¸ L âŠ™ Ïƒ'á´¸(zá´¸)
For l = L-1 to 1:
    Î´Ë¡ = (WË¡âºÂ¹)áµ€Î´Ë¡âºÂ¹ âŠ™ Ïƒ'Ë¡(zË¡)
    âˆ‚L/âˆ‚WË¡ = (1/m) * Î´Ë¡(aË¡â»Â¹)áµ€
    âˆ‚L/âˆ‚bË¡ = (1/m) * Î£Î´Ë¡
```

### Computational Graph

**Example for 2-layer network:**
```
x â†’ [WÂ¹, bÂ¹] â†’ zÂ¹ â†’ [ÏƒÂ¹] â†’ h â†’ [WÂ², bÂ²] â†’ zÂ² â†’ [ÏƒÂ²] â†’ Å· â†’ [Loss] â†’ L
```

**Backpropagation traces backward:**
```
L â†’ âˆ‚L/âˆ‚Å· â†’ âˆ‚L/âˆ‚zÂ² â†’ âˆ‚L/âˆ‚WÂ², âˆ‚L/âˆ‚bÂ²
         â†’ âˆ‚L/âˆ‚h â†’ âˆ‚L/âˆ‚zÂ¹ â†’ âˆ‚L/âˆ‚WÂ¹, âˆ‚L/âˆ‚bÂ¹
```

### Matrix Dimensions

**For a network with:**
- Input size: `nâ‚€`
- Hidden layer sizes: `nâ‚, nâ‚‚, ..., nâ‚—â‚‹â‚`
- Output size: `nâ‚—`
- Batch size: `m`

**Weight Matrices:**
- `WÂ¹`: (nâ‚ Ã— nâ‚€)
- `WÂ²`: (nâ‚‚ Ã— nâ‚)
- ...
- `Wá´¸`: (nâ‚— Ã— nâ‚—â‚‹â‚)

**Activations:**
- `Aâ°`: (m Ã— nâ‚€)
- `AÂ¹`: (m Ã— nâ‚)
- ...
- `Aá´¸`: (m Ã— nâ‚—)

**Gradients:**
- `âˆ‚L/âˆ‚WË¡`: (nË¡ Ã— nË¡â»Â¹)
- `âˆ‚L/âˆ‚bË¡`: (nË¡,)
- `Î´Ë¡`: (m Ã— nË¡)

---

## ğŸŸ¨ 6. Regularization Techniques

Regularization prevents overfitting by constraining model complexity.

### 6.1 L2 Regularization (Weight Decay)

**Modified Loss Function:**
```
L_reg = L + (Î»/2) * ||W||Â²
```

Where:
- `L` = original loss
- `Î»` = regularization strength (hyperparameter)
- `||W||Â² = Î£áµ¢â±¼ WÂ²áµ¢â±¼` = sum of squared weights

**Gradient Update:**
```
âˆ‚L_reg/âˆ‚W = âˆ‚L/âˆ‚W + Î»W
```

**Weight Update:**
```
W := W - Î±(âˆ‚L/âˆ‚W + Î»W)
    = (1 - Î±Î»)W - Î± * âˆ‚L/âˆ‚W
```

**Properties:**
- Penalizes large weights
- Encourages smooth functions
- Prevents overfitting
- `(1 - Î±Î»)` term causes weight decay

### 6.2 L1 Regularization (Lasso)

**Modified Loss Function:**
```
L_reg = L + Î» * ||W||â‚
```

Where `||W||â‚ = Î£áµ¢â±¼ |Wáµ¢â±¼|` = sum of absolute weights.

**Gradient Update:**
```
âˆ‚L_reg/âˆ‚W = âˆ‚L/âˆ‚W + Î» * sign(W)
```

**Properties:**
- Encourages sparsity (many weights become exactly zero)
- Feature selection effect
- More aggressive than L2

### 6.3 Dropout

**During Training:**
- Randomly set a fraction `p` of neurons to zero
- Each neuron is kept with probability `(1-p)`
- Scales remaining activations by `1/(1-p)`

**Mathematical Formulation:**
```
r ~ Bernoulli(1-p)
Ã£ = r âŠ™ a / (1-p)
```

Where:
- `r` = binary mask
- `a` = original activation
- `Ã£` = masked activation

**During Inference:**
- Use all neurons (no dropout)
- Scale outputs by `(1-p)` if training scaling was used

**Properties:**
- Prevents co-adaptation of neurons
- Acts as ensemble of sub-networks
- Effective regularization technique
- Common `p` values: 0.2-0.5

### 6.4 Early Stopping

**Algorithm:**
1. Split data into training and validation sets
2. Monitor validation loss during training
3. Stop training when validation loss stops improving
4. Restore weights from best validation performance

**Properties:**
- Simple and effective
- Prevents overfitting
- No additional computational cost during inference

### 6.5 Batch Normalization

**Normalization:**
```
Î¼ = (1/m) * Î£xáµ¢
ÏƒÂ² = (1/m) * Î£(xáµ¢ - Î¼)Â²
xÌ‚ = (x - Î¼) / âˆš(ÏƒÂ² + Îµ)
```

**Scale and Shift:**
```
y = Î³xÌ‚ + Î²
```

Where `Î³` and `Î²` are learnable parameters.

**Properties:**
- Normalizes activations
- Allows higher learning rates
- Reduces internal covariate shift
- Acts as regularization

---

## ğŸŸ© 7. Optimization Algorithms

### 7.1 Stochastic Gradient Descent (SGD)

**Update Rule:**
```
Î¸ := Î¸ - Î± * âˆ‡Î¸ L(Î¸; xáµ¢, yáµ¢)
```

**Properties:**
- Updates after each sample (or mini-batch)
- Noisy gradients (helps escape local minima)
- Fast convergence
- May oscillate near optimum

### 7.2 Batch Gradient Descent

**Update Rule:**
```
Î¸ := Î¸ - Î± * (1/m) * Î£âˆ‡Î¸ L(Î¸; xáµ¢, yáµ¢)
```

**Properties:**
- Uses all training data
- Smooth gradients
- Slow for large datasets
- Guaranteed convergence (for convex functions)

### 7.3 Mini-batch Gradient Descent

**Update Rule:**
```
Î¸ := Î¸ - Î± * (1/b) * Î£âˆ‡Î¸ L(Î¸; xáµ¢, yáµ¢)
```

Where `b` is the batch size.

**Properties:**
- Balance between SGD and batch GD
- Most common in practice
- Typical batch sizes: 32, 64, 128, 256

### 7.4 Momentum

**Update Rule:**
```
v := Î²v + (1-Î²) * âˆ‡Î¸ L
Î¸ := Î¸ - Î± * v
```

Where:
- `v` = velocity (exponentially weighted average of gradients)
- `Î²` = momentum coefficient (typically 0.9)

**Properties:**
- Accumulates gradient history
- Reduces oscillations
- Faster convergence
- Helps escape local minima

### 7.5 RMSprop

**Update Rule:**
```
s := Î²s + (1-Î²) * (âˆ‡Î¸ L)Â²
Î¸ := Î¸ - Î± * âˆ‡Î¸ L / (âˆšs + Îµ)
```

Where:
- `s` = exponentially weighted average of squared gradients
- `Î²` = decay rate (typically 0.9)
- `Îµ` = small constant (e.g., 1e-8)

**Properties:**
- Adapts learning rate per parameter
- Reduces oscillations in directions with large gradients
- Good for non-stationary objectives

### 7.6 Adam (Adaptive Moment Estimation)

**Update Rule:**
```
m := Î²â‚m + (1-Î²â‚) * âˆ‡Î¸ L      (first moment)
v := Î²â‚‚v + (1-Î²â‚‚) * (âˆ‡Î¸ L)Â²   (second moment)
mÌ‚ := m / (1 - Î²â‚áµ—)            (bias correction)
vÌ‚ := v / (1 - Î²â‚‚áµ—)             (bias correction)
Î¸ := Î¸ - Î± * mÌ‚ / (âˆšvÌ‚ + Îµ)
```

Where:
- `m` = first moment (mean)
- `v` = second moment (variance)
- `Î²â‚` = first moment decay (typically 0.9)
- `Î²â‚‚` = second moment decay (typically 0.999)
- `t` = iteration number

**Properties:**
- Combines Momentum and RMSprop
- Adaptive learning rates
- Bias correction for early iterations
- Most popular optimizer in practice
- Default hyperparameters work well

### 7.7 Learning Rate Scheduling

**Fixed Learning Rate:**
- Constant `Î±` throughout training

**Step Decay:**
```
Î± = Î±â‚€ * Î³^(floor(epoch / step_size))
```

**Exponential Decay:**
```
Î± = Î±â‚€ * e^(-k * epoch)
```

**Cosine Annealing:**
```
Î± = Î±_min + (Î±_max - Î±_min) * (1 + cos(Ï€ * epoch / T)) / 2
```

---

## ğŸ“Š Summary: Algorithm Comparison

| Algorithm | Update Rule | Key Features | Use Cases |
|-----------|-------------|--------------|-----------|
| **SGD** | `Î¸ := Î¸ - Î±âˆ‡L` | Simple, noisy | Large datasets, online learning |
| **Momentum** | `v := Î²v + (1-Î²)âˆ‡L; Î¸ := Î¸ - Î±v` | Reduces oscillations | Faster convergence |
| **RMSprop** | `s := Î²s + (1-Î²)(âˆ‡L)Â²; Î¸ := Î¸ - Î±âˆ‡L/âˆšs` | Adaptive per-parameter LR | Non-stationary objectives |
| **Adam** | Combines momentum + RMSprop | Adaptive, bias correction | **Default choice** for most cases |

---

## ğŸ¯ Best Practices

1. **Initialization:**
   - Use Xavier/Glorot for tanh/sigmoid
   - Use He initialization for ReLU
   - Avoid initializing all weights to zero

2. **Activation Functions:**
   - Use ReLU or variants for hidden layers
   - Use task-specific activations for output layer

3. **Regularization:**
   - Start with L2 regularization
   - Add dropout for deeper networks
   - Use early stopping

4. **Optimization:**
   - Start with Adam optimizer
   - Use learning rate scheduling
   - Monitor training/validation loss

5. **Architecture:**
   - Start simple, then increase complexity
   - Use batch normalization for deeper networks
   - Consider residual connections for very deep networks

---

## ğŸ”¬ Mathematical Foundations

### Chain Rule

**For composite functions:**
```
If z = f(y) and y = g(x), then:
dz/dx = (dz/dy) * (dy/dx)
```

**For backpropagation:**
```
âˆ‚L/âˆ‚WË¡ = (âˆ‚L/âˆ‚aË¡) * (âˆ‚aË¡/âˆ‚zË¡) * (âˆ‚zË¡/âˆ‚WË¡)
```

### Universal Approximation Theorem

**Statement:**
A feedforward neural network with a single hidden layer containing a finite number of neurons can approximate any continuous function on a compact subset of â„â¿, given appropriate activation functions and weights.

**Implications:**
- Neural networks are universal function approximators
- One hidden layer is theoretically sufficient
- In practice, deeper networks are often more efficient

---

## ğŸ“š Key Takeaways

1. **Linear Regression:** Foundation, convex optimization
2. **Logistic Regression:** Binary classification with probabilistic outputs
3. **Perceptron:** Simple binary classifier, converges for linearly separable data
4. **MLP:** Multi-layer networks with backpropagation
5. **Backpropagation:** Efficient gradient computation using chain rule
6. **Regularization:** Prevents overfitting (L1, L2, Dropout, Early Stopping)
7. **Optimization:** Various algorithms (SGD, Momentum, Adam) for efficient training

**Next Steps:**
- Convolutional Neural Networks (CNNs) for image data
- Recurrent Neural Networks (RNNs) for sequential data
- Advanced architectures (ResNet, Transformer, etc.)

